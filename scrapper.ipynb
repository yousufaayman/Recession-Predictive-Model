{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predective Model for Recessions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchWindowException, StaleElementReferenceException\n",
    "#Threading library\n",
    "import threading\n",
    "#Data Manipulation Modules\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "#NLP and Sentiment Anlaysis Modules\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#Visualization Modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "#SkLearn modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#FireBase\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore, initialize_app\n",
    "#Other Modules\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import validators\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize Firebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<firebase_admin.App at 0x1bdcb512c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cred = credentials.Certificate(\"recession-predictive-model-firebase-adminsdk-d4ulv-1673bdb119.json\")\n",
    "initialize_app(cred)\n",
    "fireDB = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Scrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = threading.Lock()\n",
    "\n",
    "def twitterScrapper(link, credentials, searchFilter, dataSet):    \n",
    "    driverOpts = Options()\n",
    "\n",
    "    driverOpts.headless = True\n",
    "    \n",
    "    driver = webdriver.Chrome(options=driverOpts)\n",
    "\n",
    "    driver.get(link)\n",
    "    try:\n",
    "        # Login to twitter\n",
    "        waitToLoad = WebDriverWait(driver, 20)\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[autocomplete=\"username\"]'))).send_keys(credentials[0])\n",
    "        driver.find_element(By.XPATH, (\"//*[contains(text(), 'Next')]\")).click();\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[autocomplete=\"current-password\"]'))).send_keys(credentials[1])\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div[data-testid='LoginForm_Login_Button'][role='button']\"))).click()\n",
    "        \n",
    "        #Add Search Filter\n",
    "        searchBar = waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[data-testid='SearchBox_Search_Input'][role='combobox']\")))\n",
    "        searchBar.send_keys(searchFilter)\n",
    "        searchBar.send_keys(Keys.ENTER)\n",
    "        \n",
    "        # Sort by Latest\n",
    "        latestTab = waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div[2]/nav/div/div[2]/div/div[2]//*[contains(text(), 'Latest')]\")))\n",
    "        latestTab.click()\n",
    "        \n",
    "        #Scrapping Tweets\n",
    "        scrappedUserNames = []\n",
    "        scrappedTweets = []\n",
    "        scrappedTweetDate = []\n",
    "\n",
    "        prevHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "\n",
    "            users = driver.find_elements(By.XPATH, (\"//div[@data-testid='User-Name']/div[1]/div[1]/a\"))\n",
    "            tweets = driver.find_elements(By.XPATH, (\"//div[@data-testid='tweetText']\"))\n",
    "            tweetDates = driver.find_elements(By.XPATH, (\"//div[@data-testid='User-Name']/div[2]/div[1]/div[3]/a/time\"))\n",
    "            \n",
    "            for user in users:\n",
    "                scrappedUserNames.append(user.text)\n",
    "                \n",
    "            for tweet in tweets:\n",
    "                scrappedTweets.append(tweet.text)\n",
    "                \n",
    "            for date in tweetDates:\n",
    "                scrappedTweetDate.append(date.text)\n",
    "\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            currentHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "            if (currentHeight == prevHeight):\n",
    "                break\n",
    "            \n",
    "            prevHeight = currentHeight\n",
    "            \n",
    "    except NoSuchWindowException:\n",
    "          print(\"Selenium window closed unexpectedly.\")\n",
    "    \n",
    "    except StaleElementReferenceException:\n",
    "            pass\n",
    "        \n",
    "    finally:\n",
    "        driver.close()\n",
    "        with lock:\n",
    "            for username, tweet, date in zip(scrappedUserNames, scrappedTweets, scrappedTweetDate):\n",
    "                dataSet[username] = [tweet, date]\n",
    "        print(\"Scrapping is Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Threads Initalization and Scrapper Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(prompt):\n",
    "    while True:\n",
    "        dateDtring = input(prompt)\n",
    "        try:\n",
    "            return datetime.datetime.strptime(dateDtring, '%Y-%m-%d').date()\n",
    "        except ValueError:\n",
    "            print(\"Invalid date format. Please use YYYY-MM-DD format.\")\n",
    "\n",
    "startDate = getDate(\"Enter the start date (YYYY-MM-DD): \")\n",
    "endDate = getDate(\"Enter the end date (YYYY-MM-DD): \")\n",
    "\n",
    "twitterCredentials =[[\"ScrpperR16322\", \"Test8dcln\"], [\"TrentWaree30151\", \"Yam8dcln\"]]\n",
    "linkTwitter = \"https://twitter.com/i/flow/login\"\n",
    "scrappedTwitterData = {}\n",
    "searchFilter = [\n",
    "    f'(\"business confidence\" OR \"economic growth\" OR \"fiscal policy\" OR \"monetary policy\" OR \"interest rates\" OR \"income inequality\" OR \"financial stability\" OR \"labor market\" OR \"economic indicators\" OR \"economic recovery\" OR \"cost of living\") until:{endDate} since:{startDate} -filter:links -filter:replies -from:MARKET_JP',\n",
    "    f'(\"economy\" OR \"economic\" OR \"job market\" OR \"unemployment\" OR \"inflation\" OR \"recession\" OR \"stock market\" OR \"GDP\" OR \"consumer spending\") until:{endDate} since:{startDate} -filter:links -filter:replies -from:MARKET_JP'\n",
    "]   \n",
    "\n",
    "threads=[]\n",
    "\n",
    "for i in range(len(twitterCredentials)):\n",
    "        th = threading.Thread(target=twitterScrapper, args=(linkTwitter, twitterCredentials[i], searchFilter[i], scrappedTwitterData))\n",
    "        threads.append(th)\n",
    "        th.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset From Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A local copy of the dataset is created and a cloud copy is added to firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "filename = \"recession_scrapped_dataset.csv\"\n",
    "datasetPath = os.path.join(currentDirectory, filename)\n",
    "\n",
    "try:\n",
    "    if not scrappedTwitterData:\n",
    "        raise ValueError(\"No Scrapped Dataset Found, Please scrap the data using the cells above before attempting to create save the data\")\n",
    "\n",
    "    newDf = pd.DataFrame({\n",
    "        'username': list(scrappedTwitterData.keys()),\n",
    "        'post': [v[0] for v in scrappedTwitterData.values()],\n",
    "        'date': [v[1] for v in scrappedTwitterData.values()],\n",
    "        'platform': ['Twitter'] * len(scrappedTwitterData)\n",
    "    })\n",
    "\n",
    "    if os.path.exists(datasetPath):\n",
    "        existingDf = pd.read_csv(datasetPath)\n",
    "        concatenatedDf = pd.concat([existingDf, newDf], ignore_index=True)\n",
    "        concatenatedDf.to_csv(datasetPath, index=False)\n",
    "        print(f\"Data saved in a new versioned file at {datasetPath}\")\n",
    "    else:\n",
    "        newDf.to_csv(datasetPath, index=False)\n",
    "        print(f\"Data saved at {datasetPath}\")\n",
    "\n",
    "    print(\"\\nPreview of the dataset:\")\n",
    "    print(newDf.head())\n",
    "\n",
    "    try:\n",
    "        for index, row in newDf.iterrows():\n",
    "            docRefrence = fireDB.collection('recession_data').document() \n",
    "            docRefrence.set(row.to_dict())\n",
    "        print(\"Data successfully uploaded to Firestore.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Firestore upload failed: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facebook Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidDate(text):\n",
    "    try:\n",
    "        datetime.strptime(text, '%d %B %Y')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "   \n",
    "def isNotUrl(url):\n",
    "    return not validators.url(url)\n",
    "\n",
    "def facebookScrapper(link, searchKeywords, credentials, dataSet, searchDates):\n",
    "     driverOpts = Options()\n",
    "\n",
    "     driverOpts.headless = False\n",
    "\n",
    "     driver = webdriver.Chrome(options=driverOpts)\n",
    "\n",
    "     driver.get(link)\n",
    "     try:\n",
    "          # Login to facebook\n",
    "          waitToLoad = WebDriverWait(driver, 20)\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//input[@data-testid='royal_email']\"))).send_keys(credentials[0])\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//input[@data-testid='royal_pass']\"))).send_keys(credentials[1])\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-testid='royal_login_button']\"))).click()\n",
    "          \n",
    "          scrappedFacebookUserNames = []\n",
    "          scrappedFacebookPosts = []\n",
    "          scrappedFacebookDate = []\n",
    "          \n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"/html\"))).click()\n",
    "          \n",
    "          # Scrapping\n",
    "          for i in range(len(searchKeywords)):\n",
    "               time.sleep(5)                    \n",
    "               #Searching\n",
    "               searchFacebook = waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[placeholder=\"Search Facebook\"]')))                  \n",
    "               searchFacebook.send_keys(Keys.CONTROL + \"a\")\n",
    "               searchFacebook.send_keys(Keys.BACKSPACE)\n",
    "               searchFacebook.send_keys(searchKeywords[i])\n",
    "               searchFacebook.send_keys(Keys.ENTER)\n",
    "               \n",
    "               for date in searchDates:\n",
    "                    time.sleep(2)                    \n",
    "                    #Selecting Search Criteria\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(),'Posts')]\"))).click()\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(),'Date posted')]\"))).click()\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, f\"//span[contains(text(),'{date}')]\"))).click()\n",
    "                    \n",
    "                    #Selecting and Storing information\n",
    "                    prevHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "                    while True:\n",
    "                         duplicateDetected = False\n",
    "                         time.sleep(5)\n",
    "\n",
    "                         fbUsers = driver.find_elements(By.XPATH, \"//a[contains(@class, 'x1i10hfl') and contains(@class, 'xjbqb8w') and contains(@class, 'x1ejq31n') and contains(@class, 'xd10rxx') and contains(@class, 'x1sy0etr') and contains(@class, 'x17r0tee') and contains(@class, 'x972fbf') and contains(@class, 'xcfux6l') and contains(@class, 'x1qhh985') and contains(@class, 'xm0m39n') and contains(@class, 'x9f619') and contains(@class, 'x1ypdohk') and contains(@class, 'xt0psk2') and contains(@class, 'xe8uvvx') and contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'xexx8yu') and contains(@class, 'x4uap5') and contains(@class, 'x18d9i69') and contains(@class, 'xkhd6sd') and contains(@class, 'x16tdsg8') and contains(@class, 'x1hl2dhg') and contains(@class, 'xggy1nq') and contains(@class, 'x1a2a7pz') and contains(@class, 'xt0b8zv') and contains(@class, 'xzsf02u') and contains(@class, 'x1s688f')]\")\n",
    "                         fbDates = driver.find_elements(By.XPATH, \"//a[contains(@class, 'x1i10hfl') and contains(@class, 'xjbqb8w') and contains(@class, 'x1ejq31n') and contains(@class, 'xd10rxx') and contains(@class, 'x1sy0etr') and contains(@class, 'x17r0tee') and contains(@class, 'x972fbf') and contains(@class, 'xcfux6l') and contains(@class, 'x1qhh985') and contains(@class, 'xm0m39n') and contains(@class, 'x9f619') and contains(@class, 'x1ypdohk') and contains(@class, 'xt0psk2') and contains(@class, 'xe8uvvx') and contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'xexx8yu') and contains(@class, 'x4uap5') and contains(@class, 'x18d9i69') and contains(@class, 'xkhd6sd') and contains(@class, 'x16tdsg8') and contains(@class, 'x1hl2dhg') and contains(@class, 'xggy1nq') and contains(@class, 'x1a2a7pz') and contains(@class, 'x1heor9g') and contains(@class, 'xt0b8zv') and contains(@class, 'xo1l8bm')]\")\n",
    "                         fbPosts = fbPosts = driver.find_elements(By.XPATH, \"//div[contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'x1vvkbs')]\")\n",
    "\n",
    "                         for post in fbPosts:\n",
    "                              if post in scrappedFacebookPosts:\n",
    "                                   duplicateDetected = True\n",
    "                                   break\n",
    "                              else:\n",
    "                                   scrappedFacebookPosts.append(post.text) \n",
    "                         \n",
    "                         if duplicateDetected:\n",
    "                              break\n",
    "                         \n",
    "                         for date in fbDates:\n",
    "                              if isValidDate(date.text):\n",
    "                                  scrappedFacebookDate.append(date.text) \n",
    "\n",
    "                         for user in fbUsers:\n",
    "                              if user.text != \"See more\" and isNotUrl(user.text):\n",
    "                                  scrappedFacebookUserNames.append(user.text) \n",
    "\n",
    "                         driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                         currentHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "                         if (currentHeight == prevHeight) or (len(scrappedFacebookPosts) > 200):\n",
    "                              break\n",
    "                         \n",
    "                         prevHeight = currentHeight\n",
    "                         \n",
    "                         print(scrappedFacebookUserNames)\n",
    "                         print(scrappedFacebookPosts)\n",
    "                         print(scrappedFacebookDate)\n",
    "               \n",
    "     except NoSuchWindowException:\n",
    "          print(\"Selenium window closed unexpectedly.\")\n",
    "          \n",
    "     print(len(scrappedFacebookPosts))\n",
    "     print(len(scrappedFacebookUserNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facebook Threads Initalization and Scrapper Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkFacebook = \"https://www.facebook.com/\"\n",
    "searchKeywords = ['economy', 'economic', 'job market', 'unemployment', 'inflation', 'recession', 'stock market', 'GDP', 'consumer spending', 'business confidence', 'economic growth', 'fiscal policy', 'monetary policy', 'interest rates', 'income inequality', 'financial stability', 'labor market', 'economic indicators', 'economic recovery', 'cost of living', \"job loss\", \"homeless\"]\n",
    "facebookCredentials = [\"fawdaclo@gmail.com\", \"Test8dcln\"]\n",
    "searchDates = [2007,2008]\n",
    "scrappedFacebookData = {}\n",
    "\n",
    "\n",
    "facebookScrapper(linkFacebook, searchKeywords, facebookCredentials, scrappedFacebookData, searchDates)\n",
    "\n",
    "currentDirectory = os.getcwd()\n",
    "filename = \"recession_scrapped_dataset.csv\"\n",
    "datasetPath = os.path.join(currentDirectory, filename)\n",
    "\n",
    "newDf = pd.DataFrame({\n",
    "    'username': list(scrappedFacebookData.keys()),\n",
    "    'post': [v[0] for v in scrappedFacebookData.values()],\n",
    "    'date': [v[1] for v in scrappedFacebookData.values()],\n",
    "    'platform': ['Facebook'] * len(scrappedFacebookData)\n",
    "})\n",
    "\n",
    "if os.path.exists(datasetPath):\n",
    "    existingDf = pd.read_csv(datasetPath)\n",
    "    concatenatedDf = pd.concat([existingDf, newDf], ignore_index=True)\n",
    "    concatenatedDf.to_csv(datasetPath, index=False)\n",
    "    print(f\"Data saved in a new versioned file at {datasetPath}\")\n",
    "else:\n",
    "    newDf.to_csv(datasetPath, index=False)\n",
    "    print(f\"Data saved at {datasetPath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = input(\"Enter the name of the dataset CSV file (without extension): or leave blank to use cloud data: \")\n",
    "datasetName = f\"{datasetName}.csv\"\n",
    "\n",
    "currentDirectory = os.getcwd()\n",
    "datasetPath = os.path.join(currentDirectory, datasetName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRelativeDates(text):\n",
    "    current_time = datetime.now()\n",
    "    units = {'m': 'minutes', 'h': 'hours', 'sec': 'seconds', 'd': 'days', 'w': 'weeks', 'y': 'years'}\n",
    "    match = re.match(r'(\\d+)\\s*([mhdwysec]+)', text)\n",
    "    if match:\n",
    "        amount, unit = match.groups()\n",
    "        unit = units.get(unit.rstrip('s'), unit)  # Ensure the unit is correctly identified\n",
    "        kwargs = {unit: -int(amount)}\n",
    "        return current_time + relativedelta(**kwargs)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def normalizeText(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove user mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def languageDetection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(datasetPath)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found.\")\n",
    "    exit()\n",
    "\n",
    "if 'post' not in data.columns or 'date' not in data.columns:\n",
    "    raise ValueError(\"Dataset must contain 'post' and 'date' columns.\")\n",
    "else:\n",
    "    # Correcting Date Formats\n",
    "    data['date'] = data['date'].apply(lambda x: parseRelativeDates(x) if isinstance(x, str) else x)\n",
    "    data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "    data.sort_values('date', inplace=True)\n",
    "\n",
    "    # Normalizing Text\n",
    "    data['post'] = data['post'].apply(normalizeText)\n",
    "\n",
    "    # Replacing all empty strings with NaN\n",
    "    data.replace('', np.nan, inplace=True)\n",
    "\n",
    "    # Removing all rows with empty or \"NaN\" posts Columns\n",
    "    data.dropna(subset=['post'], inplace=True)\n",
    "\n",
    "    # Detecting languages and keeping posts only written in English\n",
    "    data['language'] = data['post'].apply(languageDetection)\n",
    "    data = data[data['language'] == 'en']\n",
    "\n",
    "    # Removing Duplicate Rows & Columns\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    data.to_csv(datasetPath, index=False)\n",
    "    \n",
    "    collection_ref = fireDB.collection('processed_recession_data')\n",
    "    \n",
    "    try:\n",
    "        for index, row in data.iterrows():\n",
    "            doc_id = f\"{row['username']}_{row['date'].isoformat()}\"\n",
    "            doc_ref = collection_ref.document(doc_id)\n",
    "            doc_ref.set(row.to_dict(), merge=True) \n",
    "        print(\"Data successfully uploaded to Firestore.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while uploading to Firestore: {e}\")\n",
    "\n",
    "    print(datasetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Regression & SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    currentDirectory = os.getcwd()\n",
    "    \n",
    "    sentimentTrainingDatasetPath = os.path.join(currentDirectory, \"sentimentTrainerDataset.csv\")\n",
    "\n",
    "    sentimentTrainingDataset = pd.read_csv(sentimentTrainingDatasetPath, encoding='ISO-8859-1', header=None, names=['sentiment', 'text'])\n",
    "\n",
    "    if sentimentTrainingDataset.empty:\n",
    "        raise ValueError(\"Sentiment training dataset is empty.\")\n",
    "\n",
    "    requiredColumns = ['sentiment', 'text']\n",
    "    if not set(requiredColumns).issubset(sentimentTrainingDataset.columns):\n",
    "        raise ValueError(\"Required columns 'sentiment' and/or 'text' are missing in the dataset.\")\n",
    "    \n",
    "    sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "    sentimentTrainingDataset['sentiment_score'] = sentimentTrainingDataset['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sentimentTrainingDataset['text'], sentimentTrainingDataset['sentiment_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=60000)\n",
    "    X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train_vectors, y_train)\n",
    "\n",
    "    print(\"Model trained successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Sentiment training dataset file not found, please make sure the dataset is in the current working directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalyzeReg(text):\n",
    "    preprocessed_text = vectorizer.transform([text])\n",
    "    sentiment_score = model.predict(preprocessed_text)\n",
    "    return sentiment_score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Vader Sentiment Analayzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalyzeVader(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing TextBlob Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeKeywords = ['unemployment', 'inflation', 'recession', 'interest rates', 'income inequality', \"market crash\", \"poor\", \"unemployed\", \"homeless\", \"struggling\", \"high prices\", \"high mortgage\", \"lost house\", \"lost home\", \"move\", \"lost\", \"bankrupt\"]\n",
    "\n",
    "def sentimentAnalyzeTextBlob(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentences = blob.sentences\n",
    "    adjusted_sentiments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentiment_score = sentence.sentiment.polarity\n",
    "        if any(negKeyword.lower() in sentence.lower() for negKeyword in negativeKeywords):\n",
    "            sentiment_score = min(sentiment_score, -0.2)\n",
    "        elif -0.05 < sentiment_score < 0.05:\n",
    "            sentiment_score = 0 \n",
    "        adjusted_sentiments.append(sentiment_score)\n",
    "    \n",
    "    if not adjusted_sentiments:\n",
    "        return 0\n",
    "    \n",
    "    return sum(adjusted_sentiments) / len(adjusted_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testText = \"I lost my job\"\n",
    "\n",
    "print(sentimentAnalyzeReg(testText))\n",
    "print(sentimentAnalyzeVader(testText))\n",
    "print(sentimentAnalyzeTextBlob(testText))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(datasetPath)\n",
    "\n",
    "data['sentiment_score'] = data['post'].apply(sentimentAnalyzeVader) # Choose Model for Sentiment Anlayisis and add it here\n",
    "\n",
    "data.to_csv(datasetPath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Snetiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentimentTrainingDataset.empty:\n",
    "    raise ValueError(\"No data was found Please run the above Blocks.\")\n",
    "\n",
    "sns.histplot(data['sentiment_score'], bins=20, kde=True)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='date', y='sentiment_score', data=data)\n",
    "plt.title('Sentiment Score Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveTexts = data[data['sentiment_score'] > 0.1]\n",
    "positiveString = ' '.join(positiveTexts['post'].tolist())\n",
    "negativeTexts = data[data['sentiment_score'] < 0.1]\n",
    "negativeString = ' '.join(negativeTexts['post'].tolist())\n",
    "\n",
    "positiveWordCloud = WordCloud(background_color='white', max_words=200).generate(positiveString)\n",
    "negativeWordCloud = WordCloud(background_color='white', max_words=200).generate(negativeString)\n",
    "\n",
    "print(\"Positive Word Cloud\")\n",
    "plt.imshow(positiveWordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Negative Word Cloud\")\n",
    "plt.imshow(negativeWordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Dataset\n",
    "This will add a \"recession\" column to the dataset with boolean valuse which will be true if the tweet was published during a period of recession, and false if it wasn't according to the timeline of the 2008 recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "analyzedDatasetPath = os.path.join(currentDirectory, 'training_sentiment_analysis.xlsx')\n",
    "\n",
    "df = pd.read_excel(analyzedDatasetPath)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "startDate = pd.to_datetime('2007-12-01')\n",
    "endDate = pd.to_datetime('2009-01-01')\n",
    "\n",
    "df['recession'] = df['date'].apply(lambda x: startDate <= x <= endDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model for Predicting Recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['sentiment_score']] \n",
    "y = df['recession']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "predictRecessionModel = RandomForestClassifier()\n",
    "predictRecessionModel.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "modelAccuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {modelAccuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Applying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRecessionProbability(df, predictRecessionModel):\n",
    "    X_new = df[['sentiment_score']]\n",
    "\n",
    "    predictedRecession = predictRecessionModel.predict_proba(X_new)[:, 1]\n",
    "\n",
    "    recessionProbability = np.mean(predictedRecession)\n",
    "    \n",
    "    return recessionProbability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "testingDatasetPath = os.path.join(currentDirectory, 'testing_sentiment_analysis.xlsx')\n",
    "\n",
    "\n",
    "new_data = pd.read_excel(testingDatasetPath) \n",
    "\n",
    "recessionProbability = calculateRecessionProbability(new_data, predictRecessionModel)\n",
    "\n",
    "print(\"Overall probability of recession:\", recessionProbability)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
