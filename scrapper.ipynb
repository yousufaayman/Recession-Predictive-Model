{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predective Model for Recessions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchWindowException, StaleElementReferenceException\n",
    "\n",
    "#Threading library\n",
    "import threading\n",
    "#Data Manipulation Modules\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "#NLP and Sentiment Anlaysis Modules\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#Visualization Modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "#SkLearn modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Other Modules\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTargetDataset (outputFileName):\n",
    "    currentDirectory = os.getcwd()\n",
    "    targetDataset = int(input(\"To continue with working dataset enter (0) or to select Custom Dataset enter (1)\"))\n",
    "    global datasetPath\n",
    "    \n",
    "    if targetDataset == 0:\n",
    "        \n",
    "        if (datasetPath == \"\"):\n",
    "            print(\"There is no working dataset, please rerun block, choose (1), and enter dataset name\")\n",
    "            exit()\n",
    "        \n",
    "        if scrapperInit == 0:\n",
    "            outputDatasetPath = os.path.join(currentDirectory, f'training_{outputFileName}.xlsx')\n",
    "        else:\n",
    "            outputDatasetPath = os.path.join(currentDirectory, f'testing_{outputFileName}.xlsx')\n",
    "        \n",
    "        del targetDataset \n",
    "        return outputDatasetPath\n",
    "           \n",
    "    elif targetDataset == 1:\n",
    "\n",
    "        targetDataset = input(\"Enter Dataset Name (without extension): \") + \".xlsx\"\n",
    "        datasetPath = os.path.join(currentDirectory, targetDataset)\n",
    "        \n",
    "        if ((targetDataset.split('_', 1)[0]) == \"training\"):\n",
    "            outputDatasetPath = os.path.join(currentDirectory, f'training_{outputFileName}.xlsx')\n",
    "        elif ((targetDataset.split('_', 1)[0]) == \"testing\"):\n",
    "            outputDatasetPath = os.path.join(currentDirectory, f'testing_{outputFileName}.xlsx')\n",
    "        else:\n",
    "            print(\"Invalid Dataset or Dataset Name\")\n",
    "            \n",
    "        del targetDataset \n",
    "        return outputDatasetPath\n",
    "    \n",
    "scrapperInit = int(input(\"Scrap for training (0), Scrap for testing (1): \"))\n",
    "\n",
    "directory = os.getcwd()\n",
    "datasetNames = ['training_scrapped_dataset', 'testing_scrapped_dataset'] \n",
    "extension = '.xlsx'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "if scrapperInit == 0:\n",
    "    datasetName= datasetNames[0]\n",
    "elif scrapperInit == 1:\n",
    "    datasetName= datasetNames[1]\n",
    "else:\n",
    "    print(\"Please select one of the provided options as a (0) or a (1)!!\")\n",
    "    exit()\n",
    "\n",
    "datasetFiles = [f for f in files if f.startswith(datasetName) and f.endswith(extension)]\n",
    "\n",
    "if datasetFiles:\n",
    "    datasetFiles.sort(key=lambda x: int(re.search(r'\\(v(\\d+)\\)', x).group(1)) if re.search(r'\\(v(\\d+)\\)', x) else 0, reverse=True)\n",
    "    latestFile = datasetFiles[0]\n",
    "    datasetPath = os.path.join(directory, latestFile)\n",
    "else:\n",
    "    datasetPath = None\n",
    "\n",
    "if datasetPath and os.path.exists(datasetPath):\n",
    "    df = pd.read_excel(datasetPath)\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    lowestDate = df['date'].min()\n",
    "    \n",
    "    adjustedDate = lowestDate - timedelta(days=1)\n",
    "else:\n",
    "    datasetPath = os.path.join(directory, datasetName)\n",
    "    \n",
    "    if scrapperInit == 0 :\n",
    "            adjustedDate = datetime(2009, 1, 1)\n",
    "    else:\n",
    "            adjustedDate = datetime.now()\n",
    "        \n",
    "\n",
    "adjustedDateString = adjustedDate.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Scrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = threading.Lock()\n",
    "\n",
    "def twitterScrapper(link, credentials, searchFilter, dataSet):    \n",
    "    driverOpts = Options()\n",
    "\n",
    "    driverOpts.headless = True\n",
    "    \n",
    "    driver = webdriver.Chrome(options=driverOpts)\n",
    "\n",
    "    driver.get(link)\n",
    "    try:\n",
    "        # Login to twitter\n",
    "        waitToLoad = WebDriverWait(driver, 20)\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[autocomplete=\"username\"]'))).send_keys(credentials[0])\n",
    "        driver.find_element(By.XPATH, (\"//*[contains(text(), 'Next')]\")).click();\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[autocomplete=\"current-password\"]'))).send_keys(credentials[1])\n",
    "        waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"div[data-testid='LoginForm_Login_Button'][role='button']\"))).click()\n",
    "        \n",
    "        #Add Search Filter\n",
    "        searchBar = waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[data-testid='SearchBox_Search_Input'][role='combobox']\")))\n",
    "        searchBar.send_keys(searchFilter)\n",
    "        searchBar.send_keys(Keys.ENTER)\n",
    "        \n",
    "        # Sort by Latest\n",
    "        latestTab = waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[1]/div[1]/div[2]/nav/div/div[2]/div/div[2]//*[contains(text(), 'Latest')]\")))\n",
    "        latestTab.click()\n",
    "        \n",
    "        #Scrapping Tweets\n",
    "        scrappedUserNames = []\n",
    "        scrappedTweets = []\n",
    "        scrappedTweetDate = []\n",
    "\n",
    "        prevHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "\n",
    "            users = driver.find_elements(By.XPATH, (\"//div[@data-testid='User-Name']/div[1]/div[1]/a\"))\n",
    "            tweets = driver.find_elements(By.XPATH, (\"//div[@data-testid='tweetText']\"))\n",
    "            tweetDates = driver.find_elements(By.XPATH, (\"//div[@data-testid='User-Name']/div[2]/div[1]/div[3]/a/time\"))\n",
    "            \n",
    "            for user in users:\n",
    "                scrappedUserNames.append(user.text)\n",
    "                \n",
    "            for tweet in tweets:\n",
    "                scrappedTweets.append(tweet.text)\n",
    "                \n",
    "            for date in tweetDates:\n",
    "                scrappedTweetDate.append(date.text)\n",
    "\n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            currentHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "            if (currentHeight == prevHeight):\n",
    "                break\n",
    "            \n",
    "            prevHeight = currentHeight\n",
    "            \n",
    "    except NoSuchWindowException:\n",
    "          print(\"Selenium window closed unexpectedly.\")\n",
    "    \n",
    "    except StaleElementReferenceException:\n",
    "            pass\n",
    "        \n",
    "    finally:\n",
    "        driver.close()\n",
    "        with lock:\n",
    "            for username, tweet, date in zip(scrappedUserNames, scrappedTweets, scrappedTweetDate):\n",
    "                dataSet[username] = [tweet, date]\n",
    "        print(\"Scrapping is Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads Initalization and Scrapper Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterCredentials =[[\"ScrpperR16322\", \"Test8dcln\"], [\"TrentWaree30151\", \"Yam8dcln\"]]\n",
    "linkTwitter = \"https://twitter.com/i/flow/login\"\n",
    "scrappedTwitterData = {}\n",
    "\n",
    "if scrapperInit == 0:\n",
    "    searchFilter = [\n",
    "        f'(\"business confidence\" OR \"economic growth\" OR \"fiscal policy\" OR \"monetary policy\" OR \"interest rates\" OR \"income inequality\" OR \"financial stability\" OR \"labor market\" OR \"economic indicators\" OR \"economic recovery\" OR \"cost of living\") until:{adjustedDateString} since:2007-01-01 -filter:links -filter:replies -from:MARKET_JP',\n",
    "        f'(\"economy\" OR \"economic\" OR \"job market\" OR \"unemployment\" OR \"inflation\" OR \"recession\" OR \"stock market\" OR \"GDP\" OR \"consumer spending\") until:{adjustedDateString} since:2007-01-01 -filter:links -filter:replies -from:MARKET_JP'\n",
    "    ]\n",
    "    \n",
    "    print(\"Search Start Date: \", adjustedDateString)\n",
    "    \n",
    "elif scrapperInit == 1:\n",
    "    searchFilter = [\n",
    "        f'(\"business confidence\" OR \"economic growth\" OR \"fiscal policy\" OR \"monetary policy\" OR \"interest rates\" OR \"income inequality\" OR \"financial stability\" OR \"labor market\" OR \"economic indicators\" OR \"economic recovery\" OR \"cost of living\") until:{adjustedDateString} -filter:links -filter:replies -from:MARKET_JP',\n",
    "        f'(\"economy\" OR \"economic\" OR \"job market\" OR \"unemployment\" OR \"inflation\" OR \"recession\" OR \"stock market\" OR \"GDP\" OR \"consumer spending\") until:{adjustedDateString} -filter:links -filter:replies -from:MARKET_JP'\n",
    "    ]\n",
    "\n",
    "threads=[]\n",
    "\n",
    "for i in range(len(twitterCredentials)):\n",
    "        th = threading.Thread(target=twitterScrapper, args=(linkTwitter, twitterCredentials[i], searchFilter[i], scrappedTwitterData))\n",
    "        threads.append(th)\n",
    "        th.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidDate(text):\n",
    "    try:\n",
    "        datetime.strptime(text, '%d %B %Y')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "   \n",
    "def isNotUrl(url):\n",
    "    return not validators.url(url)\n",
    "\n",
    "def facebookScrapper(link, searchKeywords, credentials, dataSet, searchDates):\n",
    "     driverOpts = Options()\n",
    "\n",
    "     driverOpts.headless = False\n",
    "\n",
    "     driver = webdriver.Chrome(options=driverOpts)\n",
    "\n",
    "     driver.get(link)\n",
    "     try:\n",
    "          # Login to facebook\n",
    "          waitToLoad = WebDriverWait(driver, 20)\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//input[@data-testid='royal_email']\"))).send_keys(credentials[0])\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//input[@data-testid='royal_pass']\"))).send_keys(credentials[1])\n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//button[@data-testid='royal_login_button']\"))).click()\n",
    "          \n",
    "          scrappedFacebookUserNames = []\n",
    "          scrappedFacebookPosts = []\n",
    "          scrappedFacebookDate = []\n",
    "          \n",
    "          waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"/html\"))).click()\n",
    "          \n",
    "          # Scrapping\n",
    "          for i in range(len(searchKeywords)):\n",
    "               time.sleep(5)                    \n",
    "               #Searching\n",
    "               searchFacebook = waitToLoad.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '[placeholder=\"Search Facebook\"]')))                  \n",
    "               searchFacebook.send_keys(Keys.CONTROL + \"a\")\n",
    "               searchFacebook.send_keys(Keys.BACKSPACE)\n",
    "               searchFacebook.send_keys(searchKeywords[i])\n",
    "               searchFacebook.send_keys(Keys.ENTER)\n",
    "               \n",
    "               for date in searchDates:\n",
    "                    time.sleep(2)                    \n",
    "                    #Selecting Search Criteria\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(),'Posts')]\"))).click()\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(),'Date posted')]\"))).click()\n",
    "                    waitToLoad.until(EC.element_to_be_clickable((By.XPATH, f\"//span[contains(text(),'{date}')]\"))).click()\n",
    "                    \n",
    "                    #Selecting and Storing information\n",
    "                    prevHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "                    while True:\n",
    "                         duplicateDetected = False\n",
    "                         time.sleep(5)\n",
    "\n",
    "                         fbUsers = driver.find_elements(By.XPATH, \"//a[contains(@class, 'x1i10hfl') and contains(@class, 'xjbqb8w') and contains(@class, 'x1ejq31n') and contains(@class, 'xd10rxx') and contains(@class, 'x1sy0etr') and contains(@class, 'x17r0tee') and contains(@class, 'x972fbf') and contains(@class, 'xcfux6l') and contains(@class, 'x1qhh985') and contains(@class, 'xm0m39n') and contains(@class, 'x9f619') and contains(@class, 'x1ypdohk') and contains(@class, 'xt0psk2') and contains(@class, 'xe8uvvx') and contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'xexx8yu') and contains(@class, 'x4uap5') and contains(@class, 'x18d9i69') and contains(@class, 'xkhd6sd') and contains(@class, 'x16tdsg8') and contains(@class, 'x1hl2dhg') and contains(@class, 'xggy1nq') and contains(@class, 'x1a2a7pz') and contains(@class, 'xt0b8zv') and contains(@class, 'xzsf02u') and contains(@class, 'x1s688f')]\")\n",
    "                         fbDates = driver.find_elements(By.XPATH, \"//a[contains(@class, 'x1i10hfl') and contains(@class, 'xjbqb8w') and contains(@class, 'x1ejq31n') and contains(@class, 'xd10rxx') and contains(@class, 'x1sy0etr') and contains(@class, 'x17r0tee') and contains(@class, 'x972fbf') and contains(@class, 'xcfux6l') and contains(@class, 'x1qhh985') and contains(@class, 'xm0m39n') and contains(@class, 'x9f619') and contains(@class, 'x1ypdohk') and contains(@class, 'xt0psk2') and contains(@class, 'xe8uvvx') and contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'xexx8yu') and contains(@class, 'x4uap5') and contains(@class, 'x18d9i69') and contains(@class, 'xkhd6sd') and contains(@class, 'x16tdsg8') and contains(@class, 'x1hl2dhg') and contains(@class, 'xggy1nq') and contains(@class, 'x1a2a7pz') and contains(@class, 'x1heor9g') and contains(@class, 'xt0b8zv') and contains(@class, 'xo1l8bm')]\")\n",
    "                         fbPosts = fbPosts = driver.find_elements(By.XPATH, \"//div[contains(@class, 'xdj266r') and contains(@class, 'x11i5rnm') and contains(@class, 'xat24cr') and contains(@class, 'x1mh8g0r') and contains(@class, 'x1vvkbs')]\")\n",
    "\n",
    "                         for post in fbPosts:\n",
    "                              if post in scrappedFacebookPosts:\n",
    "                                   duplicateDetected = True\n",
    "                                   break\n",
    "                              else:\n",
    "                                   scrappedFacebookPosts.append(post.text) \n",
    "                         \n",
    "                         if duplicateDetected:\n",
    "                              break\n",
    "                         \n",
    "                         for date in fbDates:\n",
    "                              if isValidDate(date.text):\n",
    "                                  scrappedFacebookDate.append(date.text) \n",
    "\n",
    "                         for user in fbUsers:\n",
    "                              if user.text != \"See more\" and isNotUrl(user.text):\n",
    "                                  scrappedFacebookUserNames.append(user.text) \n",
    "\n",
    "                         driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                         currentHeight = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "                         if (currentHeight == prevHeight) or (len(scrappedFacebookPosts) > 200):\n",
    "                              break\n",
    "                         \n",
    "                         prevHeight = currentHeight\n",
    "                         \n",
    "                         print(scrappedFacebookUserNames)\n",
    "                         print(scrappedFacebookPosts)\n",
    "                         print(scrappedFacebookDate)\n",
    "               \n",
    "     except NoSuchWindowException:\n",
    "          print(\"Selenium window closed unexpectedly.\")\n",
    "          \n",
    "     print(len(scrappedFacebookPosts))\n",
    "     print(len(scrappedFacebookUserNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkFacebook = \"https://www.facebook.com/\"\n",
    "searchKeywords = ['economy', 'economic', 'job market', 'unemployment', 'inflation', 'recession', 'stock market', 'GDP', 'consumer spending', 'business confidence', 'economic growth', 'fiscal policy', 'monetary policy', 'interest rates', 'income inequality', 'financial stability', 'labor market', 'economic indicators', 'economic recovery', 'cost of living', \"job loss\", \"homeless\"]\n",
    "facebookCredentials = [\"fawdaclo@gmail.com\", \"Test8dcln\"]\n",
    "searchDates = [2007,2008]\n",
    "scrappedFacebookData = {}\n",
    "\n",
    "\n",
    "facebookScrapper(linkFacebook, searchKeywords, facebookCredentials, scrappedFacebookData, searchDates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSV File From Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scrappedTwitterData))\n",
    "print(datasetPath)\n",
    "\n",
    "def parseRelativeDates(text):\n",
    "    current_time = datetime.now()\n",
    "    units = {'m': 'minutes', 'h': 'hours', 's': 'seconds', 'd': 'days', 'w': 'weeks', 'y': 'years'}\n",
    "    match = re.match(r'(\\d+)([smh])', text)\n",
    "    if match:\n",
    "        amount, unit = match.groups()\n",
    "        kwargs = {units[unit]: -int(amount)}\n",
    "        return current_time + relativedelta(**kwargs)\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "def generateNewVersionPath(base_path, extension):\n",
    "    version = 2\n",
    "    base_path_without_extension, _ = os.path.splitext(base_path)\n",
    "    \n",
    "    new_path = f\"{base_path_without_extension} (v{version}){extension}\"\n",
    "    while os.path.exists(new_path):\n",
    "        version += 1\n",
    "        new_path = f\"{base_path_without_extension} (v{version}){extension}\"\n",
    "    return new_path\n",
    "\n",
    "currentDirectory = os.getcwd()\n",
    "extension = \".xlsx\"\n",
    "\n",
    "# Generate the DataFrame\n",
    "newDf = pd.DataFrame({\n",
    "    'username': list(scrappedTwitterData.keys()),\n",
    "    'post': [v[0] for v in scrappedTwitterData.values()],\n",
    "    'date': [v[1] for v in scrappedTwitterData.values()],\n",
    "    'platform': ['Twitter'] * len(scrappedTwitterData)\n",
    "})\n",
    "\n",
    "newDf['date'] = newDf['date'].apply(lambda x: parseRelativeDates(x) if isinstance(x, str) else x)\n",
    "newDf['date'] = pd.to_datetime(newDf['date'], errors='coerce')\n",
    "newDf.sort_values('date', inplace=True)\n",
    "\n",
    "if os.path.exists(datasetPath):\n",
    "    existingDf = pd.read_excel(datasetPath)\n",
    "    concatenatedDf = pd.concat([existingDf, newDf], ignore_index=True)\n",
    "    datasetPath = generateNewVersionPath(datasetPath, extension)\n",
    "    concatenatedDf.to_excel(datasetPath, index=False)\n",
    "    print(f\"Data saved in a new versioned file at {datasetPath}\")\n",
    "else:\n",
    "    newDf.to_excel(datasetPath, index=False)\n",
    "    print(f\"Data saved at {datasetPath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cleanedDatasetPath = findTargetDataset(\"dataset_cleaned\")\n",
    "except NameError:\n",
    "    print(\"Run the Preperation Block\")\n",
    "    \n",
    "#Language Detection Fucntion\n",
    "def languageDetection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "#Normalizing and Cleaning Text Data\n",
    "def normalizeText(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove user mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "#Parse Relative Dates to Normalize Dates\n",
    "def parseRelativeDates(text):\n",
    "    current_time = datetime.now()\n",
    "    units = {'m': 'minutes', 'h': 'hours', 'sec': 'seconds', 'd': 'days', 'w': 'weeks', 'y': 'years'}\n",
    "    match = re.match(r'(\\d+)\\s*([mhdwysec]+)', text)\n",
    "    if match:\n",
    "        amount, unit = match.groups()\n",
    "        unit = units.get(unit.rstrip('s'), unit)\n",
    "        kwargs = {unit: -int(amount)}\n",
    "        return current_time + relativedelta(**kwargs)\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "data = pd.read_excel(datasetPath)\n",
    "\n",
    "#Correcting Date Formats\n",
    "data['date'] = data['date'].apply(lambda x: parseRelativeDates(x) if isinstance(x, str) else x)\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "data.sort_values('date', inplace=True)\n",
    "\n",
    "#Normalizing Text\n",
    "data['post'] = data['post'].apply(normalizeText)\n",
    "\n",
    "#Replacing all empty strings with NaN\n",
    "data.replace('', np.nan, inplace=True)\n",
    "\n",
    "#Removing all rows with empty or \"NaN\" posts Columns\n",
    "data.dropna(subset=['post'], inplace=True)\n",
    "\n",
    "#Detecting languages and keeping posts only written in english\n",
    "data['language'] = data['post'].apply(lambda x: languageDetection(x))\n",
    "\n",
    "data = data[data['language'] == 'en']\n",
    "\n",
    "#Removing Duplicate Rows & Columns\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data.to_excel(cleanedDatasetPath, index=False)\n",
    "\n",
    "print(datasetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Regression & SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the first column is named 'sentiment' and the second is named 'text'\n",
    "# If your CSV doesn't have a header, you can assign column names manually\n",
    "currentDirectory = os.getcwd()\n",
    "sentimentTrainingDatasetPath = os.path.join(currentDirectory, \"sentimentTrainerDataset.csv\")\n",
    "\n",
    "# Read the CSV and specify column names if the CSV doesn't have a header row\n",
    "sentimentTrainingDataset = pd.read_csv(sentimentTrainingDatasetPath, encoding='ISO-8859-1', header=None, names=['sentiment', 'text'])\n",
    "\n",
    "# Map sentiment labels to numerical scores\n",
    "sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "sentimentTrainingDataset['sentiment_score'] = sentimentTrainingDataset['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Split the dataset for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentimentTrainingDataset['text'], sentimentTrainingDataset['sentiment_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=60000)\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalyzeReg(text):\n",
    "    preprocessed_text = vectorizer.transform([text])\n",
    "    sentiment_score = model.predict(preprocessed_text)\n",
    "    return sentiment_score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Vader Sentiment Analayzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalyzeVader(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing TextBlob Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeKeywords = ['unemployment', 'inflation', 'recession', 'interest rates', 'income inequality', \"market crash\", \"poor\", \"unemployed\", \"homeless\", \"struggling\", \"high prices\", \"high mortgage\", \"lost house\", \"lost home\", \"move\", \"lost\", \"bankrupt\"]\n",
    "\n",
    "def sentimentAnalyzeTextBlob(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentences = blob.sentences\n",
    "    adjusted_sentiments = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentiment_score = sentence.sentiment.polarity\n",
    "        if any(negKeyword.lower() in sentence.lower() for negKeyword in negativeKeywords):\n",
    "            sentiment_score = min(sentiment_score, -0.2)\n",
    "        elif -0.05 < sentiment_score < 0.05:\n",
    "            sentiment_score = 0 \n",
    "        adjusted_sentiments.append(sentiment_score)\n",
    "    \n",
    "    if not adjusted_sentiments:\n",
    "        return 0\n",
    "    \n",
    "    return sum(adjusted_sentiments) / len(adjusted_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testText = \"I lost my job\"\n",
    "\n",
    "print(sentimentAnalyzeReg(testText))\n",
    "print(sentimentAnalyzeVader(testText))\n",
    "print(sentimentAnalyzeTextBlob(testText))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "\n",
    "if scrapperInit == 0:\n",
    "    analyzedDatasetPath = os.path.join(currentDirectory, 'training_sentiment_analysis.xlsx')\n",
    "else:\n",
    "    analyzedDatasetPath = os.path.join(currentDirectory, 'testing_sentiment_analysis.xlsx')\n",
    "\n",
    "data = pd.read_excel(cleanedDatasetPath)\n",
    "\n",
    "data['sentiment_score'] = data['post'].apply(sentimentAnalyzeVader) # Choose Model for Sentiment Anlayisis and add it here\n",
    "\n",
    "data.to_excel(analyzedDatasetPath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Snetiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(analyzedDatasetPath)\n",
    "\n",
    "sns.histplot(data['sentiment_score'], bins=20, kde=True)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(x='date', y='sentiment_score', data=data)\n",
    "plt.title('Sentiment Score Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveTexts = data[data['sentiment_score'] > 0.1]\n",
    "positiveString = ' '.join(positiveTexts['post'].tolist())\n",
    "negativeTexts = data[data['sentiment_score'] < 0.1]\n",
    "negativeString = ' '.join(negativeTexts['post'].tolist())\n",
    "\n",
    "positiveWordCloud = WordCloud(background_color='white', max_words=200).generate(positiveString)\n",
    "negativeWordCloud = WordCloud(background_color='white', max_words=200).generate(negativeString)\n",
    "\n",
    "print(\"Positive Word Cloud\")\n",
    "plt.imshow(positiveWordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Negative Word Cloud\")\n",
    "plt.imshow(negativeWordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Dataset\n",
    "This will add a \"recession\" column to the dataset with boolean valuse which will be true if the tweet was published during a period of recession, and false if it wasn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "analyzedDatasetPath = os.path.join(currentDirectory, 'recession_sentiment_analysis.xlsx')\n",
    "\n",
    "data = pd.read_excel(analyzedDatasetPath)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "startDate = pd.to_datetime('2007-12-01')\n",
    "endDate = pd.to_datetime('2009-01-01')\n",
    "\n",
    "df['recession'] = df['date'].apply(lambda x: startDate <= x <= endDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model for Predicting Recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = data[['sentiment']] \n",
    "y = data['recession']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "predictRecessionModel = RandomForestClassifier()\n",
    "predictRecessionModel.fit(X_train, y_train)\n",
    "\n",
    "predictRecessionTest = np.mean(predictRecessionModel.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Applying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRecessionProbability(df, predictRecessionModel):\n",
    "    X_new = df[['sentiment']]\n",
    "\n",
    "    predictedRecession = predictRecessionModel.predict_proba(X_new)[:, 1]\n",
    "\n",
    "    recessionProbability = np.mean(predictedRecession)\n",
    "    \n",
    "    return recessionProbability\n",
    "\n",
    "new_data = ... \n",
    "\n",
    "recessionProbability = calculateRecessionProbability(new_data, predictRecessionModel)\n",
    "\n",
    "print(\"Overall probability of recession:\", recessionProbability)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
